{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7cdd8f7",
   "metadata": {},
   "source": [
    "# 실험 설계를 위한 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c63cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "original_dataset_dir = './dataset'\n",
    "# listdir()메서드는 해당 경로 하위에 있는 모든 폴더의 목록을 가져오는 메서드\n",
    "# 이번 경우에는 폴더 이름이 클래스의 이름에 해당됨\n",
    "classes_list = os.listdir(original_dataset_dir)\n",
    "\n",
    "# 나눈 데이터를 저장할 폴더를 생성\n",
    "base_dir = './splitted'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "# 분리 후에 각 데이터를 저장할 하위 폴더들 생성\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'val')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "# train, val, test 하위에 각각 클래스 목록을 생성\n",
    "for clss in classes_list:\n",
    "    os.mkdir(os.path.join(train_dir, clss))\n",
    "    os.mkdir(os.path.join(validation_dir, clss))\n",
    "    os.mkdir(os.path.join(test_dir, clss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26234b3d",
   "metadata": {},
   "source": [
    "# 데이터 분할과 클래스별 데이터 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974cedd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size( Apple___Apple_scab ):  378\n",
      "Validation size( Apple___Apple_scab ):  126\n",
      "Test size( Apple___Apple_scab ):  126\n",
      "Train size( Apple___Black_rot ):  372\n",
      "Validation size( Apple___Black_rot ):  124\n",
      "Test size( Apple___Black_rot ):  124\n",
      "Train size( Apple___Cedar_apple_rust ):  165\n",
      "Validation size( Apple___Cedar_apple_rust ):  55\n",
      "Test size( Apple___Cedar_apple_rust ):  55\n",
      "Train size( Apple___healthy ):  987\n",
      "Validation size( Apple___healthy ):  329\n",
      "Test size( Apple___healthy ):  329\n",
      "Train size( Cherry___healthy ):  512\n",
      "Validation size( Cherry___healthy ):  170\n",
      "Test size( Cherry___healthy ):  170\n",
      "Train size( Cherry___Powdery_mildew ):  631\n",
      "Validation size( Cherry___Powdery_mildew ):  210\n",
      "Test size( Cherry___Powdery_mildew ):  210\n",
      "Train size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  307\n",
      "Validation size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
      "Test size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
      "Train size( Corn___Common_rust ):  715\n",
      "Validation size( Corn___Common_rust ):  238\n",
      "Test size( Corn___Common_rust ):  238\n",
      "Train size( Corn___healthy ):  697\n",
      "Validation size( Corn___healthy ):  232\n",
      "Test size( Corn___healthy ):  232\n",
      "Train size( Corn___Northern_Leaf_Blight ):  591\n",
      "Validation size( Corn___Northern_Leaf_Blight ):  197\n",
      "Test size( Corn___Northern_Leaf_Blight ):  197\n",
      "Train size( Grape___Black_rot ):  708\n",
      "Validation size( Grape___Black_rot ):  236\n",
      "Test size( Grape___Black_rot ):  236\n",
      "Train size( Grape___Esca_(Black_Measles) ):  829\n",
      "Validation size( Grape___Esca_(Black_Measles) ):  276\n",
      "Test size( Grape___Esca_(Black_Measles) ):  276\n",
      "Train size( Grape___healthy ):  253\n",
      "Validation size( Grape___healthy ):  84\n",
      "Test size( Grape___healthy ):  84\n",
      "Train size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  645\n",
      "Validation size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
      "Test size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
      "Train size( Peach___Bacterial_spot ):  1378\n",
      "Validation size( Peach___Bacterial_spot ):  459\n",
      "Test size( Peach___Bacterial_spot ):  459\n",
      "Train size( Peach___healthy ):  216\n",
      "Validation size( Peach___healthy ):  72\n",
      "Test size( Peach___healthy ):  72\n",
      "Train size( Pepper,_bell___Bacterial_spot ):  598\n",
      "Validation size( Pepper,_bell___Bacterial_spot ):  199\n",
      "Test size( Pepper,_bell___Bacterial_spot ):  199\n",
      "Train size( Pepper,_bell___healthy ):  886\n",
      "Validation size( Pepper,_bell___healthy ):  295\n",
      "Test size( Pepper,_bell___healthy ):  295\n",
      "Train size( Potato___Early_blight ):  600\n",
      "Validation size( Potato___Early_blight ):  200\n",
      "Test size( Potato___Early_blight ):  200\n",
      "Train size( Potato___healthy ):  91\n",
      "Validation size( Potato___healthy ):  30\n",
      "Test size( Potato___healthy ):  30\n",
      "Train size( Potato___Late_blight ):  600\n",
      "Validation size( Potato___Late_blight ):  200\n",
      "Test size( Potato___Late_blight ):  200\n",
      "Train size( Strawberry___healthy ):  273\n",
      "Validation size( Strawberry___healthy ):  91\n",
      "Test size( Strawberry___healthy ):  91\n",
      "Train size( Strawberry___Leaf_scorch ):  665\n",
      "Validation size( Strawberry___Leaf_scorch ):  221\n",
      "Test size( Strawberry___Leaf_scorch ):  221\n",
      "Train size( Tomato___Bacterial_spot ):  1276\n",
      "Validation size( Tomato___Bacterial_spot ):  425\n",
      "Test size( Tomato___Bacterial_spot ):  425\n",
      "Train size( Tomato___Early_blight ):  600\n",
      "Validation size( Tomato___Early_blight ):  200\n",
      "Test size( Tomato___Early_blight ):  200\n",
      "Train size( Tomato___healthy ):  954\n",
      "Validation size( Tomato___healthy ):  318\n",
      "Test size( Tomato___healthy ):  318\n",
      "Train size( Tomato___Late_blight ):  1145\n",
      "Validation size( Tomato___Late_blight ):  381\n",
      "Test size( Tomato___Late_blight ):  381\n",
      "Train size( Tomato___Leaf_Mold ):  571\n",
      "Validation size( Tomato___Leaf_Mold ):  190\n",
      "Test size( Tomato___Leaf_Mold ):  190\n",
      "Train size( Tomato___Septoria_leaf_spot ):  1062\n",
      "Validation size( Tomato___Septoria_leaf_spot ):  354\n",
      "Test size( Tomato___Septoria_leaf_spot ):  354\n",
      "Train size( Tomato___Spider_mites Two-spotted_spider_mite ):  1005\n",
      "Validation size( Tomato___Spider_mites Two-spotted_spider_mite ):  335\n",
      "Test size( Tomato___Spider_mites Two-spotted_spider_mite ):  335\n",
      "Train size( Tomato___Target_Spot ):  842\n",
      "Validation size( Tomato___Target_Spot ):  280\n",
      "Test size( Tomato___Target_Spot ):  280\n",
      "Train size( Tomato___Tomato_mosaic_virus ):  223\n",
      "Validation size( Tomato___Tomato_mosaic_virus ):  74\n",
      "Test size( Tomato___Tomato_mosaic_virus ):  74\n",
      "Train size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  3214\n",
      "Validation size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  1071\n",
      "Test size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  1071\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "for clss in classes_list:\n",
    "    path = os.path.join(original_dataset_dir, clss)\n",
    "    # path위치에 존재하는 모든 이미지 파일의 목록을 변수 fnames에 저장\n",
    "    fnames = os.listdir(path)\n",
    "    \n",
    "    # Train, Validation, Test 데이터 비율을 지정\n",
    "    train_size = math.floor(len(fnames) * 0.6)\n",
    "    validation_size = math.floor(len(fnames) * 0.2)\n",
    "    test_size = math.floor(len(fnames) * 0.2)\n",
    "    \n",
    "    # Train 데이터에 해당하는 파일의 이름을 train_fnames에 저장\n",
    "    train_fnames = fnames[:train_size]\n",
    "    print('Train size(',clss,'): ', len(train_fnames))\n",
    "    for fname in train_fnames:\n",
    "        src = os.path.join(path, fname) # 복사할 원본 파일의 경로를 지정\n",
    "        dst = os.path.join(os.path.join(train_dir, clss), fname) # 복사한 후 저장할 파일의 경로를 지정\n",
    "        shutil.copyfile(src, dst) # src의 경로에 해당하는 파일을 dst의 경로에 저장\n",
    "        \n",
    "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
    "    print('Validation size(',clss,'): ', len(validation_fnames))\n",
    "    for fname in validation_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(validation_dir, clss), fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "    \n",
    "    test_fnames = fnames[(train_size + validation_size):(validation_size + train_size + test_size)]\n",
    "    print('Test size(',clss,'): ', len(test_fnames))\n",
    "    for fname in test_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(test_dir, clss), fname)\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4660711",
   "metadata": {},
   "source": [
    "# 베이스라인 모델 학습을 위한 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd25220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 5\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# transforms.Compose()는 이미지 전처리, Augmentataion등의 과정에서 사용되는 메서드\n",
    "# transforms.ToTensor()은 이미지를 Tensor 형태로 변환하고, 모든 값을 0~1 사이로 정규화 함\n",
    "transform_base = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\n",
    "\n",
    "# ImageFolder 메서드는 데이터셋을 불러오는 메서드. 지금 사용하는 이미지 데이터는 하나의 클래스가 하나의 폴더에 대응됨.\n",
    "# 이때, ImageFolder 메서드를 사용. transform은 이미지를 불러온 후 전처리, 또는 Augmentation을 할 방법으 지정\n",
    "train_dataset = ImageFolder(root='./splitted/train', transform=transform_base)\n",
    "val_dataset = ImageFolder(root='./splitted/val', transform=transform_base)\n",
    "\n",
    "# DataLoader은 불러온 이미지 데이터를 주어진 조건에 따라 미니 배치 단위로 분리하는 역할을 수행\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# num_workers는 데이터 프로세싱에 할당하려는 CPU코어 개수(적당한 값 설정이 필요함)\n",
    "# 코어 개수의 절반정도 수치면 무난하게 시스템 리소스를 사용하며 학습이 가능하다 함\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=3)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249fd2e",
   "metadata": {},
   "source": [
    "# 베이스라인 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1da4e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# 딥러닝 모델과 관련된 기본적인 함수를 포함하는 nn.Module 클래스를 상속\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    # __init__ 함수에서 모델에서 사용할 모든 Layer를 정의\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Conv2d(입력 채널 수, 출력 채널 수, 커널 크기)\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        # -> (64(입력 데이터의 크기) + 2 * padding(1) - 커널 크기(3)) / stride(1) + 1 = 출력 데이터의 크기(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # pooling 사이즈와 Stride는 일반적으로 같은 크기!\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        self.fc2 = nn.Linear(512, 33)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "        \n",
    "        x = x.view(-1, 4096) # Flatten 역할\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1) # 각 클래스에 속할 확률을 output으로 출력\n",
    "\n",
    "model_base = Net().to(DEVICE)\n",
    "# 본예제에서는 학습률을 0.001로 하였지만 필자의 컴퓨터 성능이 좋지 않기 때문에....\n",
    "optimizer = optim.Adam(model_base.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d89b22d",
   "metadata": {},
   "source": [
    "# 모델 학습을 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52f26455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trian_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE) # data와  target변수를 사용중인 장비에 할당\n",
    "        optimizer.zero_grad() # 이전 Batch의 Gradient값이 oprimizer에 저장되어 있으므로 optimizer를 초기화\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward() # 계산한 Loss 값을 바탕으로 Back Porpagation을 통해 계산한 Gradient값을 각각 Parameter에 할당\n",
    "        optimizer.step() # 할당된 Parameter의 Gradient값을 이용해 모델의 Parameter를 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8937dc17",
   "metadata": {},
   "source": [
    "# 모델 평가를 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c4de48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0 # 미니 배치별로 Loss를 합산해서 저장할 변수인 Test_loss를 선언하고, 0으로 초기화\n",
    "    correct = 0\n",
    "    \n",
    "    # 모델을 평가하는 단계에서 모델의 Parameter를 업데이트하지 않아야 한다\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            \n",
    "            # 모델에 입력된 Test데이터가 33개의 클래스에 속할 각각의 확률값이 Output으로 출력, 이 중 가장 높은 값을 가진\n",
    "            # 인덱스를 예측값으로 저장\n",
    "            pred = output.max(1, keepdim=True)[1] # 왜 1인지? [0]value추출 [1]index 추출\n",
    "            # view_as\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b8d95",
   "metadata": {},
   "source": [
    "# 모델 학습 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f813da87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def train_baseline(model, train_loader, val_loader, optimizer, num_epochs = 5):\n",
    "    best_acc = 0.0 # 정확도가 가장 높은 모델의 정확도를 저장하는 변수\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # 정확도가 가장 높은 모델을 저장할 변수\n",
    "    # state_dic(): 각 계층을 매개변수 텐서로 매핑되는 Python 사전(dict) 객체\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        since = time.time() # 한 Epoch당 소요되는 시간을 측정하기 위해 해당 Epoch를 시작할 때의 시각을 저장\n",
    "        train(model, train_loader, optimizer)\n",
    "        train_loss, train_acc = evaluate(model, train_loader)\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        time_elapsed = time.time() - since\n",
    "        print('--------------epoch {}-------------------'.format(epoch))\n",
    "        print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))\n",
    "        print('val Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))   \n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        # early_stopping는 validation loss가 감소하였는지 확인이 필요하며,\n",
    "        # 만약 감소하였을경우 현제 모델을 checkpoint로 만든다.\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d94efabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------epoch 1-------------------\n",
      "train Loss: 3.2850, Accuracy: 13.40%\n",
      "val Loss: 3.2847, Accuracy: 13.41%\n",
      "Completed in 2m 36s\n",
      "--------------epoch 2-------------------\n",
      "train Loss: 2.7345, Accuracy: 23.09%\n",
      "val Loss: 2.7400, Accuracy: 22.67%\n",
      "Completed in 2m 38s\n",
      "--------------epoch 3-------------------\n",
      "train Loss: 2.0407, Accuracy: 40.16%\n",
      "val Loss: 2.0556, Accuracy: 39.19%\n",
      "Completed in 2m 37s\n",
      "--------------epoch 4-------------------\n",
      "train Loss: 1.7631, Accuracy: 47.36%\n",
      "val Loss: 1.7924, Accuracy: 46.35%\n",
      "Completed in 2m 36s\n",
      "--------------epoch 5-------------------\n",
      "train Loss: 1.8319, Accuracy: 44.18%\n",
      "val Loss: 1.8687, Accuracy: 43.40%\n",
      "Completed in 2m 37s\n"
     ]
    }
   ],
   "source": [
    "base = train_baseline(model_base, train_loader, val_loader, optimizer, EPOCH)\n",
    "\n",
    "torch.save(base,'baseline.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c558587",
   "metadata": {},
   "source": [
    "*본 예제에서는 epoch = 30, learning rate = 0.001이지만  \n",
    "컴퓨터 성능이 좋지 않은 필자는 임의로 epoch = 5, learning rate = 0.01로 조정했다  \n",
    "필자는 Accuracy: 44.18% 나왔지만 원래 Accuracy > 98% 정도 나온다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164a7110",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b800f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize([64, 64]),\n",
    "        transforms.RandomHorizontalFlip(), # 좌우 반전\n",
    "        transforms.RandomVerticalFlip(), # 상하 반전\n",
    "        transforms.RandomCrop(52), # 이미지의 일부를 랜덤하게 잘라내어 52*52 사이즈로 변경\n",
    "        transforms.ToTensor(),\n",
    "        # 이미지가 Tensor 형태로 전환된 이후에 정규화를 시행. 평균값과 표준편차 값이 필요\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], # RGB 채널 값에서 정규화를 적용할 평균값\n",
    "                            [0.229, 0.224, 0.225]) # RGB 채널 값에서 정규화를 적용할 표준편차\n",
    "    ]),\n",
    "    # 학습에 사용한 Augmentation 부분을 제외한 나머지 부분을 동일하게 적용\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize([64, 64]),\n",
    "        transforms.RandomCrop(52),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a935cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df2687b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './splitted'\n",
    "image_datasets = {x:ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x]) for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers = 3)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5c10a",
   "metadata": {},
   "source": [
    "# Pre-Trained Model 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21b66135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b131f658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\choic/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "24.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "61.9%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "99.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet50(pretrained=True) # pretrained 옵션을 True로 설정하면 미리 학습된 모델의 Parameter 값을 그대로 가져옴\n",
    "# resnet의 출력 채널과 이 프로젝트의 채널수가 달라 마지막 Fully Connected Layer을 수정하기 위해 채널수를 저장\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet = resnet.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 설정한 일부 Layer의 Parameter만을 업이트 해야 하기 때문에, requires_grad = True로 설정된 Layer의 Parameter에만 적용\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr = 0.001)\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# StepLR() 메서드는 Epoch에 따라 Learning Rate를 변경하는 역할\n",
    "# 7 Epoch마다 0.1씩 곱해 Learning Rate를 감소 시킨다는 의미\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029ecb7a",
   "metadata": {},
   "source": [
    "# Pre-Trained Model의 일부 Layer Freeze하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de8d7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = 0 # 해당 Layer가 몇 번째 Layer인지를 나타내는 변수 ct의 값을 0으로 초기화\n",
    "for child in resnet.children():\n",
    "    # children() 메서드는 모델의 자식 모듈을 반복 가능한 객체로 반환하는 메서드\n",
    "    ct += 1\n",
    "    if ct < 6:\n",
    "        for param in child.parameters(): # child.parameter()는 각 Layer의 Parameter Tesnsor를 의미\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209a253",
   "metadata": {},
   "source": [
    "# Transfer Learning 모델 학습과 검증을 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "262de0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())  \n",
    "    best_acc = 0.0  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('-------------- epoch {} ----------------'.format(epoch+1)) \n",
    "        since = time.time()                                     \n",
    "        for phase in ['train', 'val']: \n",
    "            if phase == 'train': \n",
    "                model.train() \n",
    "            else:\n",
    "                model.eval()     \n",
    " \n",
    "            running_loss = 0.0  \n",
    "            running_corrects = 0  \n",
    " \n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]: \n",
    "                inputs = inputs.to(DEVICE)  \n",
    "                labels = labels.to(DEVICE)  \n",
    "                \n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):  \n",
    "                    outputs = model(inputs)  \n",
    "                    _, preds = torch.max(outputs, 1) \n",
    "                    loss = criterion(outputs, labels)  \n",
    "    \n",
    "                    if phase == 'train':   \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    " \n",
    "                running_loss += loss.item() * inputs.size(0)  \n",
    "                running_corrects += torch.sum(preds == labels.data)  \n",
    "            if phase == 'train':  \n",
    "                scheduler.step()\n",
    " \n",
    "            epoch_loss = running_loss/dataset_sizes[phase]  \n",
    "            epoch_acc = running_corrects.double()/dataset_sizes[phase]  \n",
    " \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) \n",
    " \n",
    "          \n",
    "            if phase == 'val' and epoch_acc > best_acc: \n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    " \n",
    "        time_elapsed = time.time() - since  \n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    " \n",
    "    model.load_state_dict(best_model_wts) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b78ca",
   "metadata": {},
   "source": [
    "# 모델 학습 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "caa234e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 1 ----------------\n",
      "train Loss: 0.2076 Acc: 0.9305\n",
      "val Loss: 0.2805 Acc: 0.9173\n",
      "Completed in 9m 19s\n",
      "-------------- epoch 2 ----------------\n",
      "train Loss: 0.1662 Acc: 0.9460\n",
      "val Loss: 0.2415 Acc: 0.9265\n",
      "Completed in 9m 27s\n",
      "-------------- epoch 3 ----------------\n",
      "train Loss: 0.1351 Acc: 0.9550\n",
      "val Loss: 0.1161 Acc: 0.9598\n",
      "Completed in 9m 26s\n",
      "-------------- epoch 4 ----------------\n",
      "train Loss: 0.0961 Acc: 0.9673\n",
      "val Loss: 0.1623 Acc: 0.9486\n",
      "Completed in 9m 26s\n",
      "-------------- epoch 5 ----------------\n",
      "train Loss: 0.0938 Acc: 0.9691\n",
      "val Loss: 0.1353 Acc: 0.9588\n",
      "Completed in 9m 27s\n",
      "Best val Acc: 0.959820\n"
     ]
    }
   ],
   "source": [
    "model_resnet50 = train_resnet(resnet, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH)\n",
    "\n",
    "torch.save(model_resnet50, 'resnet50.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5898e",
   "metadata": {},
   "source": [
    "# 모델평가  \n",
    "+ 베이스라이 모델 평가를 위한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5215a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_base = transforms.Compose([transforms.Resize([64,64]), transforms.ToTensor()])\n",
    "test_base = ImageFolder(root='./splitted/test', transform=transform_base)\n",
    "test_loader_base = torch.utils.data.DataLoader(test_base, batch_size=BATCH_SIZE, shuffle=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0351227",
   "metadata": {},
   "source": [
    "+ Transfer Leaning모델 평가를 위한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1f59395",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_resNet = transforms.Compose([\n",
    "    transforms.Resize([64, 64]),\n",
    "    transforms.RandomCrop(52),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.487, 0.456, 0.407], [0.229, 0.224, 0.255])\n",
    "])\n",
    "\n",
    "test_resNet = ImageFolder(root='./splitted/test', transform=transform_resNet)\n",
    "\n",
    "test_loader_resNet = torch.utils.data.DataLoader(test_resNet, batch_size=BATCH_SIZE, shuffle=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb22ed6",
   "metadata": {},
   "source": [
    "+ 베이스라인 모델 성능 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32ef4788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline test acc:   46.96457629240205\n"
     ]
    }
   ],
   "source": [
    "baseline=torch.load('baseline.pt') \n",
    "baseline.eval()  \n",
    "test_loss, test_accuracy = evaluate(baseline, test_loader_base)\n",
    "\n",
    "print('baseline test acc:  ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ef242",
   "metadata": {},
   "source": [
    "+ Transfer Learning 모델 성능 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1fcfa986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet test acc:   92.55225935661535\n"
     ]
    }
   ],
   "source": [
    "resnet50=torch.load('resnet50.pt') \n",
    "resnet50.eval()  \n",
    "test_loss, test_accuracy = evaluate(resnet50, test_loader_resNet)\n",
    "\n",
    "print('ResNet test acc:  ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380bd797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythorchDeepLearningProject",
   "language": "python",
   "name": "book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
